{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmS4Ytdkt1cvet8jJ6mLDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiramDream/pailab/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIBWyZYzZqR6",
        "outputId": "382dea9d-07cf-4312-db84-64ffd1c03d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy (Value Iteration):\n",
            "State (0, 0): right\n",
            "State (0, 1): right\n",
            "State (0, 2): right\n",
            "State (1, 0): up\n",
            "State (1, 1): up\n",
            "State (1, 2): up\n",
            "State (1, 3): up\n",
            "State (2, 0): up\n",
            "State (2, 1): right\n",
            "State (2, 2): up\n",
            "State (2, 3): up\n",
            "\n",
            "Policy (Policy Iteration):\n",
            "State (0, 0): right\n",
            "State (0, 1): right\n",
            "State (0, 2): right\n",
            "State (1, 0): up\n",
            "State (1, 1): up\n",
            "State (1, 2): up\n",
            "State (1, 3): up\n",
            "State (2, 0): up\n",
            "State (2, 1): right\n",
            "State (2, 2): up\n",
            "State (2, 3): up\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleGridWorldMDP:\n",
        "    def __init__(self, grid, terminal_states, rewards, gamma=0.99):\n",
        "        self.grid = grid\n",
        "        self.terminal_states = terminal_states\n",
        "        self.rewards = rewards\n",
        "        self.gamma = gamma\n",
        "        self.states = [(i, j) for i in range(len(grid)) for j in range(len(grid[0]))]\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.transitions = self.build_transitions()\n",
        "\n",
        "    def build_transitions(self):\n",
        "        transitions = {}\n",
        "        for state in self.states:\n",
        "            transitions[state] = {}\n",
        "            for action in self.actions:\n",
        "                next_state, reward = self.get_next_state_and_reward(state, action)\n",
        "                transitions[state][action] = (next_state, reward)\n",
        "        return transitions\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        if state in self.terminal_states:\n",
        "            return state, self.rewards.get(state, 0)\n",
        "\n",
        "        i, j = state\n",
        "        if action == 'up':\n",
        "            next_state = (max(i - 1, 0), j)\n",
        "        elif action == 'down':\n",
        "            next_state = (min(i + 1, len(self.grid) - 1), j)\n",
        "        elif action == 'left':\n",
        "            next_state = (i, max(j - 1, 0))\n",
        "        elif action == 'right':\n",
        "            next_state = (i, min(j + 1, len(self.grid[0]) - 1))\n",
        "\n",
        "        reward = self.rewards.get(next_state, 0)\n",
        "        return next_state, reward\n",
        "\n",
        "    def value_iteration(self, theta=1e-5):\n",
        "        V = {state: 0 for state in self.states}\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in self.states:\n",
        "                if state in self.terminal_states:\n",
        "                    continue\n",
        "                v = V[state]\n",
        "                V[state] = max(sum(prob * (reward + self.gamma * V[next_state])\n",
        "                                   for (next_state, reward), prob in [(self.transitions[state][a], 1.0)])\n",
        "                               for a in self.actions)\n",
        "                delta = max(delta, abs(v - V[state]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        policy = {state: self.actions[np.argmax([sum(prob * (reward + self.gamma * V[next_state])\n",
        "                                                     for (next_state, reward), prob in [(self.transitions[state][a], 1.0)])\n",
        "                                             for a in self.actions])] for state in self.states if state not in self.terminal_states}\n",
        "        return policy, V\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        policy = {state: np.random.choice(self.actions) for state in self.states if state not in self.terminal_states}\n",
        "        V = {state: 0 for state in self.states}\n",
        "\n",
        "        def policy_evaluation(policy, V):\n",
        "            while True:\n",
        "                delta = 0\n",
        "                for state in self.states:\n",
        "                    if state in self.terminal_states:\n",
        "                        continue\n",
        "                    v = V[state]\n",
        "                    action = policy[state]\n",
        "                    V[state] = sum(prob * (reward + self.gamma * V[next_state])\n",
        "                                   for (next_state, reward), prob in [(self.transitions[state][action], 1.0)])\n",
        "                    delta = max(delta, abs(v - V[state]))\n",
        "                if delta < 1e-5:\n",
        "                    break\n",
        "            return V\n",
        "\n",
        "        while True:\n",
        "            V = policy_evaluation(policy, V)\n",
        "            policy_stable = True\n",
        "            for state in self.states:\n",
        "                if state in self.terminal_states:\n",
        "                    continue\n",
        "                old_action = policy[state]\n",
        "                policy[state] = self.actions[np.argmax([sum(prob * (reward + self.gamma * V[next_state])\n",
        "                                                            for (next_state, reward), prob in [(self.transitions[state][a], 1.0)])\n",
        "                                                    for a in self.actions])]\n",
        "                if old_action != policy[state]:\n",
        "                    policy_stable = False\n",
        "            if policy_stable:\n",
        "                break\n",
        "        return policy, V\n",
        "\n",
        "\n",
        "# Example usage\n",
        "grid = [\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 0, 0]\n",
        "]\n",
        "\n",
        "terminal_states = [(0, 3)]\n",
        "rewards = {(0, 3): 1, (1, 1): -1}\n",
        "\n",
        "mdp = SimpleGridWorldMDP(grid, terminal_states, rewards)\n",
        "\n",
        "policy_vi, value_vi = mdp.value_iteration()\n",
        "print(\"Policy (Value Iteration):\")\n",
        "for state in sorted(policy_vi.keys()):\n",
        "    print(f\"State {state}: {policy_vi[state]}\")\n",
        "\n",
        "policy_pi, value_pi = mdp.policy_iteration()\n",
        "print(\"\\nPolicy (Policy Iteration):\")\n",
        "for state in sorted(policy_pi.keys()):\n",
        "    print(f\"State {state}: {policy_pi[state]}\")\n"
      ]
    }
  ]
}